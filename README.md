# SpatialMech

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone <repo-url>
cd spatialmech

# Create conda environment
conda create -n spatialmech python=3.11
conda activate spatialmech

# Install dependencies
pip install -r requirements.txt
```

### Set Environment Variables

```bash
# Set path to spatial reasoning evaluation data
export SPATIAL_DATA_DIR="/leonardo_work/EUHPC_D27_102/compmech/whatsup_vlms_data"
```
### Evaluate a VLM

```bash
python scripts/evaluate.py \
    --model-name qwen2vl-vllm \
    --dataset Controlled_Images_A \
    --device cuda
```

## ğŸ“š Key Components

### Model Zoo
Pre-trained VLMs for spatial reasoning evaluation:
- Qwen2-VL (standard & vLLM)
- LLaVA 1.5/1.6
- PaliGemma

```python
from model_zoo import get_model
model, preprocess = get_model("qwen2vl-vllm", device="cuda")
```

### Dataset Zoo
Spatial reasoning benchmarks:
- Controlled Images (A/B)
- COCO-QA (one/two objects)
- Visual Genome QA (one/two objects)
- VSR

```python
from dataset_zoo import get_dataset
dataset = get_dataset("Controlled_Images_A", root_dir="/path/to/data")
```

### Spatial Probes
Linear classifiers for understanding vision model representations:
```python
from src.models import SpatialProbe

probe = SpatialProbe(input_dim=768, num_relations=4)
```

## ğŸ“Š Workflows

### 1. Feature Extraction â†’ Probe Training

```bash
# Extract features from a vision model
python scripts/extract_features.py \
    --model dinov2 \
    --dataset datasets/spatial_hf \
    --output datasets/features/dinov2

# Train spatial probes
python scripts/train_probes.py \
    --config config/dinov2.yaml \
    --features datasets/features/dinov2

# Visualize results
python scripts/plot_results.py \
    --results results/dinov2/ \
    --output plots/dinov2_probes.png
```

### 2. VLM Evaluation

```bash
# Evaluate on multiple datasets
for dataset in Controlled_Images_A COCO_QA_one_obj VG_QA_one_obj; do
    python scripts/evaluate.py \
        --model-name qwen2vl-vllm \
        --dataset $dataset \
        --output results/qwen/
done

# Compare models
python scripts/compare_models.py \
    --results results/ \
    --models qwen2vl llava1.5 paligemma
```

### 3. Analysis

```bash
# Launch Jupyter for analysis
jupyter notebook notebooks/

# Or run specific analysis
python scripts/analyze_layers.py --results results/dinov2/
```

## ğŸ› ï¸ Available Scripts

| Script | Purpose |
|--------|---------|
| `train_probes.py` | Train spatial reasoning probes |
| `extract_features.py` | Extract features from vision models |
| `evaluate.py` | Evaluate VLMs on spatial reasoning |
| `compare_models.py` | Compare multiple models |
| `plot_results.py` | Visualize results |

Run with `--help` for detailed options.

## âš™ï¸ Configuration

Configurations are in `config/*.yaml`:

```yaml
# config/dinov2.yaml
model:
  name: "dinov2"
  checkpoint: "facebook/dinov2-large"
  layers: [10, 15, 20, 23]

dataset:
  name: "spatial_relations"
  path: "datasets/spatial_hf"
  relations: ["left", "right", "top", "bottom"]

training:
  batch_size: 32
  learning_rate: 0.001
  epochs: 100
```

## ğŸ” Key Concepts

### Evaluation vs Training

**Evaluation (model_zoo + dataset_zoo)**:
- ğŸ¯ Test full VLMs on benchmarks
- ğŸ“Š Image-text pairs
- ğŸ” Accuracy metrics

**Training (src + datasets)**:
- ğŸ¯ Train lightweight probes
- ğŸ“Š Feature vectors + labels
- ğŸ” Trained probe models

### Probes vs VLMs

**Probes**:
- Linear classifiers on frozen features
- Fast to train (~minutes)
- Show what info is encoded

**VLMs**:
- End-to-end models
- Generate natural language
- Evaluate complex reasoning

## ğŸ“¦ Dependencies

Key packages:
- PyTorch 2.0+
- Transformers
- vLLM (for fast inference)
- Datasets (HuggingFace)
- PIL/Pillow
- NumPy, Pandas
- Matplotlib, Seaborn

See `requirements.txt` for complete list.

## ğŸ“„ Citation

If you use this code, please cite:

## ğŸ“§ Contact

For questions or issues, please open a GitHub issue.

## ğŸ“œ License

[Specify your license]

---

**Quick Links**:
- ğŸ“– [Project Structure](PROJECT_STRUCTURE.md)
- ğŸ¤– [Model Zoo](model_zoo/README.md)
- ğŸ“Š [Dataset Zoo](dataset_zoo/README.md)
- ğŸ—‚ï¸ [Training Datasets](datasets/README.md)
