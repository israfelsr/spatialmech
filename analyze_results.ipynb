{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Results Analysis\n",
    "\n",
    "This notebook analyzes model evaluation results to understand:\n",
    "- Where models are making mistakes\n",
    "- Answer distribution patterns\n",
    "- Most common confusions\n",
    "- Per-class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your results file\n",
    "RESULTS_FILE = \"./output/results_paligemma_Controlled_Images_B_None_fouroption_False.json\"\n",
    "\n",
    "# You can also compare multiple models\n",
    "# RESULTS_FILES = {\n",
    "#     \"PaliGemma\": \"./output/results_paligemma_Controlled_Images_B_None_fouroption_False.json\",\n",
    "#     \"Qwen2.5-VL\": \"./output/results_qwen_vllm_Controlled_Images_B_None_fouroption_False.json\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spatial_answer(generation):\n",
    "    \"\"\"\n",
    "    Extract spatial relation from model generation.\n",
    "    \n",
    "    Args:\n",
    "        generation: Raw model output string\n",
    "    \n",
    "    Returns:\n",
    "        Extracted spatial relation (lowercase) or 'unknown'\n",
    "    \"\"\"\n",
    "    gen_lower = generation.lower().strip()\n",
    "    \n",
    "    # List of possible spatial relations\n",
    "    spatial_relations = [\n",
    "        'left', 'right', 'above', 'below', 'top', 'bottom',\n",
    "        'on', 'under', 'front', 'behind', 'in-front'\n",
    "    ]\n",
    "    \n",
    "    # Try to find exact matches first\n",
    "    for relation in spatial_relations:\n",
    "        if relation == gen_lower or f' {relation} ' in f' {gen_lower} ':\n",
    "            return relation\n",
    "    \n",
    "    # Try to find relations at the beginning or end\n",
    "    for relation in spatial_relations:\n",
    "        if gen_lower.startswith(relation + ' ') or gen_lower.endswith(' ' + relation):\n",
    "            return relation\n",
    "    \n",
    "    # If no exact match, return the generation (truncated)\n",
    "    return gen_lower[:20] if gen_lower else 'unknown'\n",
    "\n",
    "\n",
    "# Load results\n",
    "with open(RESULTS_FILE, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(results)} results\")\n",
    "print(f\"\\nFirst result example:\")\n",
    "print(json.dumps(results[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Clean Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and ground truth\n",
    "predicted_answers = []\n",
    "golden_answers = []\n",
    "correct_predictions = []\n",
    "raw_generations = []\n",
    "\n",
    "for result in results:\n",
    "    raw_gen = result['Generation']\n",
    "    pred = extract_spatial_answer(raw_gen)\n",
    "    gold = result['Golden'].lower() if isinstance(result['Golden'], str) else result['Golden'][0].lower()\n",
    "    \n",
    "    raw_generations.append(raw_gen)\n",
    "    predicted_answers.append(pred)\n",
    "    golden_answers.append(gold)\n",
    "    correct_predictions.append(pred == gold)\n",
    "\n",
    "# Create DataFrame for easier analysis\n",
    "df = pd.DataFrame({\n",
    "    'prompt': [r['Prompt'] for r in results],\n",
    "    'raw_generation': raw_generations,\n",
    "    'predicted': predicted_answers,\n",
    "    'golden': golden_answers,\n",
    "    'correct': correct_predictions\n",
    "})\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(df)\n",
    "correct = df['correct'].sum()\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {total}\")\n",
    "print(f\"Correct predictions: {correct}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class Accuracy\n",
    "\n",
    "Which spatial relations is the model struggling with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class accuracy\n",
    "per_class = df.groupby('golden').agg({\n",
    "    'correct': ['sum', 'count', 'mean']\n",
    "}).round(4)\n",
    "per_class.columns = ['correct', 'total', 'accuracy']\n",
    "per_class['accuracy'] = per_class['accuracy'] * 100\n",
    "per_class = per_class.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(per_class)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(per_class.index, per_class['accuracy'], color='steelblue', alpha=0.8)\n",
    "ax.set_xlabel('Spatial Relation (Golden Answer)', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title(f'Per-Class Accuracy\\nOverall Accuracy: {accuracy:.2f}%', fontsize=14)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.axhline(y=accuracy, color='r', linestyle='--', alpha=0.5, label=f'Overall: {accuracy:.1f}%')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, (idx, row) in zip(bars, per_class.iterrows()):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "            f'n={int(row[\"total\"])}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Distribution\n",
    "\n",
    "Compare golden vs predicted answer distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count distributions\n",
    "golden_dist = Counter(golden_answers)\n",
    "predicted_dist = Counter(predicted_answers)\n",
    "\n",
    "print(\"\\nGolden Answer Distribution:\")\n",
    "for answer, count in golden_dist.most_common():\n",
    "    pct = 100 * count / len(golden_answers)\n",
    "    print(f\"  {answer:15s}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\nPredicted Answer Distribution:\")\n",
    "for answer, count in predicted_dist.most_common():\n",
    "    pct = 100 * count / len(predicted_answers)\n",
    "    print(f\"  {answer:15s}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Visualize side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Golden distribution\n",
    "ax1.bar(golden_dist.keys(), golden_dist.values(), color='green', alpha=0.6)\n",
    "ax1.set_xlabel('Spatial Relation', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_title('Golden Answer Distribution', fontsize=14)\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Predicted distribution\n",
    "ax2.bar(predicted_dist.keys(), predicted_dist.values(), color='orange', alpha=0.6)\n",
    "ax2.set_xlabel('Spatial Relation', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Predicted Answer Distribution', fontsize=14)\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "What does the model predict when it's wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix\n",
    "all_labels = sorted(set(golden_answers + predicted_answers))\n",
    "confusion = np.zeros((len(all_labels), len(all_labels)))\n",
    "label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "\n",
    "for gold, pred in zip(golden_answers, predicted_answers):\n",
    "    confusion[label_to_idx[gold], label_to_idx[pred]] += 1\n",
    "\n",
    "# Normalize by row (golden answer) to get percentages\n",
    "confusion_norm = confusion / (confusion.sum(axis=1, keepdims=True) + 1e-10) * 100\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.heatmap(confusion_norm, annot=True, fmt='.1f', cmap='YlOrRd',\n",
    "            xticklabels=all_labels, yticklabels=all_labels,\n",
    "            cbar_kws={'label': 'Percentage (%)'}, ax=ax)\n",
    "ax.set_xlabel('Predicted Answer', fontsize=12)\n",
    "ax.set_ylabel('Golden Answer', fontsize=12)\n",
    "ax.set_title('Confusion Matrix (% of each golden answer)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also show absolute counts\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.heatmap(confusion, annot=True, fmt='.0f', cmap='Blues',\n",
    "            xticklabels=all_labels, yticklabels=all_labels,\n",
    "            cbar_kws={'label': 'Count'}, ax=ax)\n",
    "ax.set_xlabel('Predicted Answer', fontsize=12)\n",
    "ax.set_ylabel('Golden Answer', fontsize=12)\n",
    "ax.set_title('Confusion Matrix (Absolute Counts)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Confusion Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common confusions for each class\n",
    "print(\"\\nMost Common Confusion Patterns:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for gold in sorted(set(golden_answers)):\n",
    "    gold_mask = df['golden'] == gold\n",
    "    predictions = df[gold_mask]['predicted'].values\n",
    "    pred_counts = Counter(predictions)\n",
    "    \n",
    "    print(f\"\\nWhen golden answer is '{gold}' (n={sum(gold_mask)}):\")\n",
    "    for pred, count in pred_counts.most_common(5):\n",
    "        pct = 100 * count / sum(gold_mask)\n",
    "        is_correct = \"✓\" if pred == gold else \"✗\"\n",
    "        print(f\"  {is_correct} {pred:15s}: {count:4d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Specific Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View incorrect predictions\n",
    "incorrect_df = df[~df['correct']].copy()\n",
    "print(f\"\\nTotal incorrect predictions: {len(incorrect_df)}\")\n",
    "\n",
    "# Show first few errors\n",
    "print(\"\\nFirst 10 errors:\")\n",
    "incorrect_df[['prompt', 'raw_generation', 'predicted', 'golden']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Specific Confusion Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a specific confusion to investigate\n",
    "GOLD_ANSWER = 'left'  # Change this to investigate different confusions\n",
    "PRED_ANSWER = 'right'  # Change this to investigate different confusions\n",
    "\n",
    "confusion_mask = (df['golden'] == GOLD_ANSWER) & (df['predicted'] == PRED_ANSWER)\n",
    "confusion_cases = df[confusion_mask]\n",
    "\n",
    "print(f\"\\nCases where golden='{GOLD_ANSWER}' but predicted='{PRED_ANSWER}': {len(confusion_cases)}\")\n",
    "print(\"\\nExamples:\")\n",
    "for idx, row in confusion_cases.head(5).iterrows():\n",
    "    print(f\"\\nPrompt: {row['prompt']}\")\n",
    "    print(f\"Generated: {row['raw_generation']}\")\n",
    "    print(f\"Predicted: {row['predicted']} | Golden: {row['golden']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Generation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the raw generations to understand model behavior\n",
    "print(\"\\nSample raw generations (correct predictions):\")\n",
    "for idx, row in df[df['correct']].sample(min(5, len(df[df['correct']]))).iterrows():\n",
    "    print(f\"\\nGolden: {row['golden']}\")\n",
    "    print(f\"Raw: {row['raw_generation']}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n\\nSample raw generations (incorrect predictions):\")\n",
    "for idx, row in df[~df['correct']].sample(min(5, len(df[~df['correct']]))).iterrows():\n",
    "    print(f\"\\nGolden: {row['golden']} | Predicted: {row['predicted']}\")\n",
    "    print(f\"Raw: {row['raw_generation']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to JSON\n",
    "summary = {\n",
    "    'overall': {\n",
    "        'total': total,\n",
    "        'correct': int(correct),\n",
    "        'accuracy': float(accuracy)\n",
    "    },\n",
    "    'per_class_accuracy': per_class.to_dict('index'),\n",
    "    'golden_distribution': dict(golden_dist),\n",
    "    'predicted_distribution': dict(predicted_dist)\n",
    "}\n",
    "\n",
    "output_file = RESULTS_FILE.replace('.json', '_analysis.json')\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nAnalysis saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
