{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Hop Attention Hypothesis Analysis\n",
    "\n",
    "This notebook demonstrates how to analyze attention patterns to test the 2-hop hypothesis:\n",
    "\n",
    "**Hypothesis:** Text tokens gather information from the image, and the last token reads from those text tokens (rather than directly from the image).\n",
    "\n",
    "## Metrics Computed:\n",
    "\n",
    "1. **Last Token Attention Distribution**: % attending to image vs text tokens\n",
    "2. **Text Tokens Attention Distribution**: % attending to image vs other text tokens\n",
    "3. **Attention Flow Score**: Correlation between text→image and last→text\n",
    "4. **Information Bottleneck Score**: Which text tokens act as \"hubs\"\n",
    "5. **Direct vs Indirect Image Access**: Compare last→image vs last→text→image\n",
    "6. **Layer-wise Evolution**: How patterns change across layers\n",
    "7. **Positional Analysis**: Which token types attend most to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from mechanistic.utils.two_hop_metrics import (\n",
    "    compute_all_two_hop_metrics,\n",
    "    print_metrics_summary,\n",
    "    identify_token_ranges,\n",
    ")\n",
    "from mechanistic.utils.two_hop_visualization import (\n",
    "    plot_complete_analysis,\n",
    "    plot_attention_distribution_comparison,\n",
    "    plot_attention_flow,\n",
    "    plot_hub_tokens_analysis,\n",
    "    plot_direct_vs_indirect,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Your Data\n",
    "\n",
    "Load a sample that contains:\n",
    "- `attentions`: Tuple of attention tensors from each layer\n",
    "- `input_ids`: Token IDs\n",
    "- Optional: `caption`, `question`, `pred`, `GT`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load your sample data\n",
    "# Replace with your actual data path\n",
    "sample = torch.load('path/to/your/sample.pt')\n",
    "\n",
    "# Or if you have a list of samples\n",
    "# samples = torch.load('path/to/your/samples.pt')\n",
    "# sample = samples[0]\n",
    "\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Number of layers: {len(sample['attentions'])}\")\n",
    "print(f\"Attention shape: {sample['attentions'][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Tokenizer\n",
    "\n",
    "The tokenizer is needed to decode tokens and identify vision/text token boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer for your model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "# Or for Qwen3-VL:\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-VL-4B-Instruct\")\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute All Metrics\n",
    "\n",
    "This single function computes all 2-hop hypothesis metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all metrics\n",
    "metrics = compute_all_two_hop_metrics(\n",
    "    sample,\n",
    "    tokenizer=tokenizer,\n",
    "    layer_idx=-1,  # Analyze last layer\n",
    "    average_heads=True,\n",
    "    analyze_all_layers=True,  # Set to True to get layer-wise evolution\n",
    "    keywords=[\"left\", \"right\", \"front\", \"behind\", \"where\"]  # Customize for your task\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print_metrics_summary(metrics, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Results\n",
    "\n",
    "### 4.1 Attention Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_distribution_comparison(metrics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Attention Flow Diagram\n",
    "\n",
    "Shows the 2-hop path: Image → Text Tokens → Last Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention tensor and token ranges\n",
    "attention = sample['attentions'][-1]  # Last layer\n",
    "vision_range, text_range = identify_token_ranges(sample['input_ids'], tokenizer)\n",
    "\n",
    "plot_attention_flow(\n",
    "    attention,\n",
    "    vision_range,\n",
    "    text_range,\n",
    "    metrics,\n",
    "    top_k_text_tokens=5\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Hub Tokens Analysis\n",
    "\n",
    "Identifies which text tokens act as \"information hubs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hub_tokens_analysis(\n",
    "    metrics,\n",
    "    attention,\n",
    "    vision_range,\n",
    "    text_range\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Direct vs Indirect Image Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_direct_vs_indirect(metrics)\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "if metrics.indirect_to_direct_ratio > 1:\n",
    "    print(f\"✅ Supports 2-hop hypothesis: Indirect path is {metrics.indirect_to_direct_ratio:.2f}x stronger\")\n",
    "else:\n",
    "    print(f\"❌ Direct path dominates: {1/metrics.indirect_to_direct_ratio:.2f}x stronger than indirect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Generate All Plots at Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all plots to a directory\n",
    "plot_complete_analysis(\n",
    "    metrics,\n",
    "    attention,\n",
    "    vision_range,\n",
    "    text_range,\n",
    "    output_dir=\"plots/two_hop_analysis\",\n",
    "    prefix=\"sample_0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Multiple Samples\n",
    "\n",
    "Compute aggregate statistics across many samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple samples\n",
    "samples = torch.load('path/to/your/samples.pt')\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "\n",
    "# Analyze all samples\n",
    "all_metrics = []\n",
    "for i, sample in enumerate(samples[:50]):  # Analyze first 50\n",
    "    try:\n",
    "        metrics = compute_all_two_hop_metrics(\n",
    "            sample,\n",
    "            tokenizer=tokenizer,\n",
    "            layer_idx=-1,\n",
    "            analyze_all_layers=False  # Faster if you don't need layer-wise\n",
    "        )\n",
    "        all_metrics.append(metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"Error on sample {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Successfully analyzed {len(all_metrics)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Compute Aggregate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract metrics\n",
    "last_token_image_pcts = [m.last_token_image_pct for m in all_metrics]\n",
    "text_tokens_image_pcts = [m.text_tokens_image_pct_mean for m in all_metrics]\n",
    "attention_flow_scores = [m.attention_flow_score for m in all_metrics]\n",
    "hub_scores = [m.information_bottleneck_score for m in all_metrics]\n",
    "indirect_ratios = [m.indirect_to_direct_ratio for m in all_metrics]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGGREGATE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nLast Token → Image: {np.mean(last_token_image_pcts):.2f}% ± {np.std(last_token_image_pcts):.2f}%\")\n",
    "print(f\"Text Tokens → Image: {np.mean(text_tokens_image_pcts):.2f}% ± {np.std(text_tokens_image_pcts):.2f}%\")\n",
    "print(f\"\\nAttention Flow Score: {np.mean(attention_flow_scores):.3f} ± {np.std(attention_flow_scores):.3f}\")\n",
    "print(f\"Hub Score: {np.mean(hub_scores):.3f} ± {np.std(hub_scores):.3f}\")\n",
    "print(f\"\\nIndirect/Direct Ratio: {np.mean(indirect_ratios):.2f}x ± {np.std(indirect_ratios):.2f}x\")\n",
    "print(f\"% samples supporting 2-hop (ratio > 1): {np.mean([r > 1 for r in indirect_ratios]) * 100:.1f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Plot Aggregate Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Last token image attention\n",
    "axes[0, 0].hist(last_token_image_pcts, bins=30, alpha=0.7, color='#FF6B6B', edgecolor='black')\n",
    "axes[0, 0].axvline(np.mean(last_token_image_pcts), color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Last Token → Image (%)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Last Token Image Attention')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Attention flow score\n",
    "axes[0, 1].hist(attention_flow_scores, bins=30, alpha=0.7, color='#4ECDC4', edgecolor='black')\n",
    "axes[0, 1].axvline(np.mean(attention_flow_scores), color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Attention Flow Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Attention Flow Score (Correlation)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Indirect/Direct ratio\n",
    "axes[1, 0].hist(indirect_ratios, bins=30, alpha=0.7, color='#95E1D3', edgecolor='black')\n",
    "axes[1, 0].axvline(np.mean(indirect_ratios), color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].axvline(1.0, color='gray', linestyle=':', linewidth=2, label='Equal (1.0x)')\n",
    "axes[1, 0].set_xlabel('Indirect/Direct Ratio')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Indirect vs Direct Ratio')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter: Last token vs Text tokens\n",
    "axes[1, 1].scatter(text_tokens_image_pcts, last_token_image_pcts, alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Text Tokens → Image (mean %)')\n",
    "axes[1, 1].set_ylabel('Last Token → Image (%)')\n",
    "axes[1, 1].set_title('Last Token vs Text Tokens Image Attention')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'2-Hop Hypothesis: Aggregate Statistics (n={len(all_metrics)})', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpretation Guide\n",
    "\n",
    "### Strong Evidence for 2-Hop:\n",
    "- ✅ Last token has LOW image attention (< 20%)\n",
    "- ✅ Text tokens have HIGH image attention (> 40%)\n",
    "- ✅ Attention flow score is POSITIVE and HIGH (> 0.5)\n",
    "- ✅ Indirect/Direct ratio > 1 (indirect path is stronger)\n",
    "\n",
    "### Weak or No Evidence:\n",
    "- ❌ Last token has HIGH image attention (> 50%)\n",
    "- ❌ Attention flow score is NEGATIVE or LOW (< 0.2)\n",
    "- ❌ Indirect/Direct ratio < 1 (direct path dominates)\n",
    "\n",
    "### Hub Tokens:\n",
    "- Look for **object names** and **spatial relation words** acting as hubs\n",
    "- High hub scores (> 0.5) indicate strong bottleneck effect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
