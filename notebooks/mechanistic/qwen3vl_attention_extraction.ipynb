{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-VL Attention Extraction for Spatial Reasoning\n",
    "\n",
    "This notebook extracts **cross-attention** from Qwen3-VL-4B-Instruct to understand how the model attends to image regions when reasoning about spatial relations.\n",
    "\n",
    "**Key Difference from CLIP:**\n",
    "- CLIP: Only self-attention (no text → image interaction)\n",
    "- Qwen3-VL: Has cross-attention where text tokens attend to image patches\n",
    "\n",
    "**What we'll extract:**\n",
    "1. Vision encoder self-attention (image patches attending to each other)\n",
    "2. **Cross-attention** (text tokens attending to image patches) ← Most important!\n",
    "3. Language model self-attention (text attending to text + image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "from dataset_zoo.aro_datasets import get_controlled_images_b\n",
    "from mechanistic.utils.visualization import (\n",
    "    plot_attention_heatmap, \n",
    "    plot_attention_on_image,\n",
    "    plot_cross_attention_attribution\n",
    ")\n",
    "from mechanistic.utils.hooks import AttentionCache\n",
    "from mechanistic.utils.metrics import attention_entropy, attention_concentration\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Qwen3-VL Model\n",
    "\n",
    "We'll load the model with `output_attentions=True` to get attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path (update if needed)\n",
    "model_path = \"/leonardo_work/EUHPC_D27_102/compmech/models/Qwen3-VL-4B-Instruct\"\n",
    "\n",
    "print(f\"Loading Qwen3-VL from: {model_path}\")\n",
    "\n",
    "# Load model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    # output_attentions=True  # This needs to be set during forward pass\n",
    ")\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "print(f\"Model loaded successfully\")\n",
    "print(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Sample from Controlled Images B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = get_controlled_images_b(\n",
    "    image_preprocess=None,\n",
    "    download=False,\n",
    "    root_dir='/leonardo_work/EUHPC_D27_102/compmech/whatsup_vlms_data'\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Get a sample\n",
    "sample_idx = 0\n",
    "sample = dataset[sample_idx]\n",
    "\n",
    "# Extract caption and objects\n",
    "caption = sample['caption_options'][0]\n",
    "words = caption.split()\n",
    "object1 = words[1]\n",
    "object2 = words[-1]\n",
    "\n",
    "print(f\"\\nSample {sample_idx}:\")\n",
    "print(f\"Caption: {caption}\")\n",
    "print(f\"Object 1: {object1}\")\n",
    "print(f\"Object 2: {object2}\")\n",
    "\n",
    "# Display image\n",
    "image = sample['image_options'][0]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Sample Image\\n{caption}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Prompt in Qwen Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create question\n",
    "question = f\"Where is the {object1} in relation to the {object2}? Answer with left, right, front or behind.\"\n",
    "print(f\"Question: {question}\")\n",
    "\n",
    "# Prepare messages in Qwen format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(f\"\\nFormatted prompt (first 200 chars):\\n{text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process Input and Extract Attention\n",
    "\n",
    "**Important:** Qwen-VL processes images into vision tokens that get concatenated with text tokens. We need to:\n",
    "1. Identify which tokens are image tokens\n",
    "2. Extract attention from text tokens to image tokens (cross-attention pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process inputs\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Total tokens: {inputs['input_ids'].shape[1]}\")\n",
    "\n",
    "# Decode tokens to see the sequence\n",
    "tokens = processor.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print(f\"\\nFirst 20 tokens: {tokens[:20]}\")\n",
    "print(f\"Last 20 tokens: {tokens[-20:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Forward Pass with Attention Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass with attention\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **inputs,\n",
    "        output_attentions=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "print(f\"Output keys: {outputs.keys()}\")\n",
    "\n",
    "# Get attentions\n",
    "if hasattr(outputs, 'attentions') and outputs.attentions is not None:\n",
    "    attentions = outputs.attentions\n",
    "    print(f\"\\nNumber of layers with attention: {len(attentions)}\")\n",
    "    print(f\"Attention shape (first layer): {attentions[0].shape}\")\n",
    "    print(f\"Format: [batch, heads, seq_len, seq_len]\")\n",
    "else:\n",
    "    print(\"\\nWarning: No attention weights found in outputs!\")\n",
    "    print(\"This might mean the model doesn't support output_attentions or we need a different approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Identify Image vs Text Tokens\n",
    "\n",
    "Qwen-VL uses special tokens to mark image regions. We need to identify:\n",
    "- `<|vision_start|>` and `<|vision_end|>` markers\n",
    "- Image patch tokens between these markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find image token positions\n",
    "token_ids = inputs['input_ids'][0]\n",
    "tokens_list = [processor.tokenizer.decode(t) for t in token_ids]\n",
    "\n",
    "# Look for vision markers\n",
    "vision_start_token = \"<|vision_start|>\"\n",
    "vision_end_token = \"<|vision_end|>\"\n",
    "image_pad_token = \"<|image_pad|>\"\n",
    "\n",
    "# Find indices\n",
    "vision_start_idx = None\n",
    "vision_end_idx = None\n",
    "\n",
    "for i, token in enumerate(tokens_list):\n",
    "    if vision_start_token in token:\n",
    "        vision_start_idx = i\n",
    "    if vision_end_token in token:\n",
    "        vision_end_idx = i\n",
    "        break\n",
    "\n",
    "if vision_start_idx and vision_end_idx:\n",
    "    print(f\"Image tokens span: [{vision_start_idx}, {vision_end_idx}]\")\n",
    "    print(f\"Number of image tokens: {vision_end_idx - vision_start_idx - 1}\")\n",
    "    \n",
    "    # Calculate grid size (assuming square patches)\n",
    "    num_image_tokens = vision_end_idx - vision_start_idx - 1\n",
    "    grid_size = int(np.sqrt(num_image_tokens))\n",
    "    print(f\"Image grid size: {grid_size}x{grid_size}\")\n",
    "else:\n",
    "    print(\"Could not find vision tokens!\")\n",
    "    print(f\"Tokens: {tokens_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Extract Cross-Attention (Text → Image)\n",
    "\n",
    "We want to see how text tokens (especially spatial words like \"left\", \"right\") attend to image patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attentions and vision_start_idx and vision_end_idx:\n",
    "    # Get last layer attention\n",
    "    last_layer_attn = attentions[-1]  # [batch, heads, seq, seq]\n",
    "    \n",
    "    # Average over heads\n",
    "    avg_attn = last_layer_attn[0].mean(0)  # [seq, seq]\n",
    "    \n",
    "    # Extract cross-attention: text tokens → image tokens\n",
    "    # Text tokens are after vision_end_idx\n",
    "    text_start_idx = vision_end_idx + 1\n",
    "    \n",
    "    # Attention from text to image patches\n",
    "    cross_attn = avg_attn[text_start_idx:, vision_start_idx+1:vision_end_idx]\n",
    "    \n",
    "    print(f\"Cross-attention shape: {cross_attn.shape}\")\n",
    "    print(f\"Format: [num_text_tokens, num_image_patches]\")\n",
    "    \n",
    "    # Get text tokens for labeling\n",
    "    text_tokens = tokens_list[text_start_idx:]\n",
    "    print(f\"\\nText tokens: {text_tokens[:20]}...\")  # First 20 text tokens\n",
    "else:\n",
    "    print(\"Cannot extract cross-attention - missing attention or vision token indices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Cross-Attention for Key Words\n",
    "\n",
    "Let's find spatial words (left, right, front, behind) and object names in the text, then visualize where they attend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cross_attn' in locals():\n",
    "    # Find indices of interesting tokens\n",
    "    spatial_words = ['left', 'right', 'front', 'behind', 'where']\n",
    "    object_words = [object1.lower(), object2.lower()]\n",
    "    interesting_words = spatial_words + object_words\n",
    "    \n",
    "    interesting_indices = []\n",
    "    interesting_labels = []\n",
    "    \n",
    "    for i, token in enumerate(text_tokens):\n",
    "        token_clean = token.strip().lower()\n",
    "        for word in interesting_words:\n",
    "            if word in token_clean:\n",
    "                interesting_indices.append(i)\n",
    "                interesting_labels.append(f\"{token}({i})\")\n",
    "                break\n",
    "    \n",
    "    print(f\"Found {len(interesting_indices)} interesting tokens: {interesting_labels}\")\n",
    "    \n",
    "    if len(interesting_indices) > 0:\n",
    "        # Plot attention for each interesting token\n",
    "        n_tokens = min(len(interesting_indices), 6)  # Max 6 tokens\n",
    "        fig, axes = plt.subplots(1, n_tokens + 1, figsize=(4 * (n_tokens + 1), 4))\n",
    "        \n",
    "        # Plot original image\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Plot attention for each token\n",
    "        for plot_idx, token_idx in enumerate(interesting_indices[:n_tokens]):\n",
    "            token_attn = cross_attn[token_idx]  # [num_image_patches]\n",
    "            \n",
    "            # Reshape to 2D grid\n",
    "            attn_grid = token_attn.cpu().reshape(grid_size, grid_size).numpy()\n",
    "            \n",
    "            # Normalize\n",
    "            attn_grid = (attn_grid - attn_grid.min()) / (attn_grid.max() - attn_grid.min() + 1e-8)\n",
    "            \n",
    "            # Resize to image size\n",
    "            from scipy.ndimage import zoom\n",
    "            img_array = np.array(image)\n",
    "            zoom_factor = (img_array.shape[0] / grid_size, img_array.shape[1] / grid_size)\n",
    "            attn_resized = zoom(attn_grid, zoom_factor, order=1)\n",
    "            \n",
    "            # Plot\n",
    "            axes[plot_idx + 1].imshow(img_array)\n",
    "            axes[plot_idx + 1].imshow(attn_resized, cmap='hot', alpha=0.6)\n",
    "            axes[plot_idx + 1].set_title(f'Token: {interesting_labels[plot_idx]}')\n",
    "            axes[plot_idx + 1].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Cross-Attention: Text Tokens → Image Patches\\n{caption}', fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No interesting tokens found in text!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Aggregate Attention for Spatial Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cross_attn' in locals() and len(interesting_indices) > 0:\n",
    "    # Get attention for spatial words only\n",
    "    spatial_token_indices = []\n",
    "    for i, token in enumerate(text_tokens):\n",
    "        if any(word in token.lower() for word in spatial_words):\n",
    "            spatial_token_indices.append(i)\n",
    "    \n",
    "    if len(spatial_token_indices) > 0:\n",
    "        # Average attention across all spatial tokens\n",
    "        spatial_attn = cross_attn[spatial_token_indices].mean(0)  # [num_image_patches]\n",
    "        \n",
    "        # Reshape and visualize\n",
    "        attn_grid = spatial_attn.cpu().reshape(grid_size, grid_size).numpy()\n",
    "        attn_grid = (attn_grid - attn_grid.min()) / (attn_grid.max() - attn_grid.min() + 1e-8)\n",
    "        \n",
    "        # Resize\n",
    "        from scipy.ndimage import zoom\n",
    "        img_array = np.array(image)\n",
    "        zoom_factor = (img_array.shape[0] / grid_size, img_array.shape[1] / grid_size)\n",
    "        attn_resized = zoom(attn_grid, zoom_factor, order=1)\n",
    "        \n",
    "        # Plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        ax1.imshow(image)\n",
    "        ax1.set_title('Original Image')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        ax2.imshow(img_array)\n",
    "        im = ax2.imshow(attn_resized, cmap='hot', alpha=0.6)\n",
    "        ax2.set_title('Averaged Spatial Word Attention')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
    "        plt.suptitle(f'Where does the model look for spatial reasoning?\\n{caption}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No spatial words found in text!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Analyze Attention Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cross_attn' in locals():\n",
    "    # Calculate metrics for cross-attention\n",
    "    entropy = attention_entropy(cross_attn, dim=-1)\n",
    "    concentration = attention_concentration(cross_attn, k=5)\n",
    "    \n",
    "    print(f\"Cross-Attention Metrics:\")\n",
    "    print(f\"  Average entropy: {entropy.mean():.3f} (lower = more focused)\")\n",
    "    print(f\"  Average top-5 concentration: {concentration.mean():.3f} (higher = more concentrated)\")\n",
    "    \n",
    "    # Plot distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax1.hist(entropy.cpu().numpy(), bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(entropy.mean().item(), color='red', linestyle='--', label=f'Mean: {entropy.mean():.3f}')\n",
    "    ax1.set_xlabel('Attention Entropy')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Distribution of Attention Entropy\\n(per text token)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.hist(concentration.cpu().numpy(), bins=30, alpha=0.7, edgecolor='black', color='orange')\n",
    "    ax2.axvline(concentration.mean().item(), color='red', linestyle='--', label=f'Mean: {concentration.mean():.3f}')\n",
    "    ax2.set_xlabel('Top-5 Concentration')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Attention Concentration\\n(per text token)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### ✅ What This Notebook Demonstrates:\n",
    "1. Loading Qwen3-VL and extracting attention weights\n",
    "2. Identifying image vs text tokens in the sequence\n",
    "3. Extracting **cross-attention** (text → image)\n",
    "4. Visualizing where spatial words attend on the image\n",
    "5. Analyzing attention metrics (entropy, concentration)\n",
    "\n",
    "### 🚀 Next:\n",
    "1. **Test on multiple samples**\n",
    "   - Compare correct vs incorrect predictions\n",
    "   - Different spatial relations (left/right vs front/behind)\n",
    "   \n",
    "2. **Layer-wise analysis**\n",
    "   - How does cross-attention evolve across layers?\n",
    "   - Which layers are most important?\n",
    "   \n",
    "3. **Statistical analysis**\n",
    "   - Do correct predictions have more focused attention?\n",
    "   - Do spatial words consistently attend to relevant regions?\n",
    "   \n",
    "4. **Build automated pipeline**\n",
    "   - Batch processing\n",
    "   - Save attention maps\n",
    "   - Generate summary statistics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
