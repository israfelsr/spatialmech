{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Extraction Prototype\n",
    "\n",
    "This notebook validates attention extraction from vision-language models for spatial reasoning analysis.\n",
    "\n",
    "**Goals:**\n",
    "1. Extract attention weights from a VLM (starting with HuggingFace models)\n",
    "2. Visualize attention on a sample image\n",
    "3. Verify we can map attention to image regions\n",
    "\n",
    "**Models to test:**\n",
    "- Qwen2.5-VL (HuggingFace version)\n",
    "- Alternative: PaliGemma (if Qwen attention extraction is difficult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from dataset_zoo.aro_datasets import get_controlled_images_b\n",
    "from mechanistic.utils.visualization import plot_attention_heatmap, plot_attention_on_image\n",
    "from mechanistic.utils.hooks import AttentionCache\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load a Sample from Controlled Images B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = get_controlled_images_b(\n",
    "    image_preprocess=None,  # No preprocessing, we want raw images\n",
    "    download=False,\n",
    "    root_dir='../../data'\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Get a sample\n",
    "sample_idx = 0\n",
    "sample = dataset[sample_idx]\n",
    "\n",
    "print(f\"\\nSample {sample_idx}:\")\n",
    "print(f\"Caption options: {sample['caption_options']}\")\n",
    "print(f\"Correct caption: {sample['caption_options'][0]}\")\n",
    "\n",
    "# Display image\n",
    "image = sample['image_options'][0]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Sample Image\\n{sample['caption_options'][0]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Caption and Create Prompt\n",
    "\n",
    "For Controlled_B, the format is: \"A {object1} {relation} a {object2}\"\n",
    "We'll create a question asking about the spatial relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the caption to extract objects and relation\n",
    "caption = sample['caption_options'][0]\n",
    "words = caption.split()\n",
    "object1 = words[1]  # Position [1]\n",
    "object2 = words[-1]  # Position [-1]\n",
    "relation = ' '.join(words[2:-2])  # Everything in between\n",
    "\n",
    "print(f\"Object 1: {object1}\")\n",
    "print(f\"Relation: {relation}\")\n",
    "print(f\"Object 2: {object2}\")\n",
    "\n",
    "# Create a question prompt\n",
    "question = f\"Where is the {object1} in relation to the {object2}? Answer with left, right, front or behind.\"\n",
    "print(f\"\\nQuestion: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Model with Attention Output\n",
    "\n",
    "We'll start with a simpler model to validate the approach. Let's try with a vision transformer that we can easily extract attention from.\n",
    "\n",
    "**Options:**\n",
    "1. Use CLIP vision encoder (simpler, just for validation)\n",
    "2. Use Qwen2.5-VL (full VLM, but more complex)\n",
    "3. Use PaliGemma (good middle ground)\n",
    "\n",
    "Let's start with **Option 1 (CLIP)** for quick validation, then move to full VLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: CLIP Vision Transformer (for validation)\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "print(\"Loading CLIP model...\")\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name, output_attentions=True)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process inputs\n",
    "inputs = processor(\n",
    "    text=[question],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(device)\n",
    "\n",
    "# Forward pass with attention\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "# Get vision attention weights\n",
    "vision_attentions = outputs.vision_model_output.attentions\n",
    "print(f\"Number of vision layers: {len(vision_attentions)}\")\n",
    "print(f\"Attention shape (first layer): {vision_attentions[0].shape}\")\n",
    "print(f\"Format: [batch, heads, seq_len, seq_len]\")\n",
    "\n",
    "# Get text attention weights  \n",
    "text_attentions = outputs.text_model_output.attentions\n",
    "print(f\"\\nNumber of text layers: {len(text_attentions)}\")\n",
    "print(f\"Text attention shape (first layer): {text_attentions[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Attention from Last Layer\n",
    "\n",
    "The last layer attention is typically most interpretable for the final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last layer vision attention\n",
    "last_layer_attn = vision_attentions[-1]  # [1, heads, seq_len, seq_len]\n",
    "print(f\"Last layer attention shape: {last_layer_attn.shape}\")\n",
    "\n",
    "# Average over heads\n",
    "avg_attn = last_layer_attn[0].mean(0)  # [seq_len, seq_len]\n",
    "print(f\"Averaged attention shape: {avg_attn.shape}\")\n",
    "\n",
    "# For vision transformers, seq_len = num_patches + 1 (CLS token)\n",
    "# CLIP uses 32x32 patches for 224x224 images = 7x7 = 49 patches + 1 CLS = 50 tokens\n",
    "num_patches = int(np.sqrt(avg_attn.shape[0] - 1))\n",
    "print(f\"Number of patches per dimension: {num_patches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Attention Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attention heatmap\n",
    "fig, ax = plot_attention_heatmap(\n",
    "    avg_attn,\n",
    "    layer_name=\"CLIP Vision (Last Layer)\",\n",
    "    figsize=(10, 8)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Overlay Attention on Image\n",
    "\n",
    "We'll visualize how the CLS token attends to different image patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CLS token attention to patches (row 0, columns 1:)\n",
    "cls_to_patches = avg_attn[0, 1:]  # [num_patches^2]\n",
    "\n",
    "# Normalize for visualization\n",
    "cls_to_patches = (cls_to_patches - cls_to_patches.min()) / (cls_to_patches.max() - cls_to_patches.min())\n",
    "\n",
    "# Overlay on image\n",
    "fig, ax = plot_attention_on_image(\n",
    "    image,\n",
    "    cls_to_patches,\n",
    "    num_patches=num_patches,\n",
    "    alpha=0.6,\n",
    "    cmap='hot'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Multiple Heads\n",
    "\n",
    "Different attention heads may focus on different aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mechanistic.utils.visualization import plot_multi_head_attention\n",
    "\n",
    "# Plot all heads from last layer\n",
    "fig, axes = plot_multi_head_attention(\n",
    "    last_layer_attn,\n",
    "    layer_name=\"CLIP Vision (Last Layer)\",\n",
    "    max_heads=12,\n",
    "    figsize=(20, 16)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Analyze Attention Across Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mechanistic.utils.metrics import attention_entropy, attention_concentration\n",
    "\n",
    "# Calculate metrics for each layer\n",
    "entropies = []\n",
    "concentrations = []\n",
    "\n",
    "for layer_idx, attn in enumerate(vision_attentions):\n",
    "    # Average over heads and batch\n",
    "    avg_layer_attn = attn[0].mean(0)\n",
    "    \n",
    "    # Calculate entropy (how focused?)\n",
    "    entropy = attention_entropy(avg_layer_attn, dim=-1).mean().item()\n",
    "    entropies.append(entropy)\n",
    "    \n",
    "    # Calculate concentration (top-5 sum)\n",
    "    concentration = attention_concentration(avg_layer_attn, k=5).mean().item()\n",
    "    concentrations.append(concentration)\n",
    "\n",
    "# Plot metrics across layers\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(entropies, marker='o')\n",
    "ax1.set_xlabel('Layer')\n",
    "ax1.set_ylabel('Attention Entropy')\n",
    "ax1.set_title('Attention Entropy Across Layers')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(concentrations, marker='o', color='orange')\n",
    "ax2.set_xlabel('Layer')\n",
    "ax2.set_ylabel('Top-5 Concentration')\n",
    "ax2.set_title('Attention Concentration Across Layers')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average entropy: {np.mean(entropies):.3f}\")\n",
    "print(f\"Average concentration: {np.mean(concentrations):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### ✅ What We've Validated:\n",
    "1. We can extract attention weights from vision transformers\n",
    "2. We can visualize attention as heatmaps\n",
    "3. We can overlay attention on images\n",
    "4. We can calculate attention metrics (entropy, concentration)\n",
    "\n",
    "### 🚀 Next:\n",
    "1. **Adapt for Qwen2.5-VL or PaliGemma**\n",
    "   - These models have full VLM capabilities\n",
    "   - Extract cross-attention (text → image)\n",
    "   \n",
    "2. **Run on Multiple Samples**\n",
    "   - Correct vs incorrect predictions\n",
    "   - Different spatial relations\n",
    "   \n",
    "3. **Build Analysis Pipeline**\n",
    "   - Automated extraction\n",
    "   - Statistical analysis\n",
    "   - Summary visualizations\n",
    "\n",
    "### 📝 Notes for Full VLM:\n",
    "- Need to identify cross-attention layers (text attending to image)\n",
    "- May need to use non-vLLM versions for attention access\n",
    "- Consider using Captum or other attribution libraries for more advanced analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
