{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Results Analysis\n",
    "\n",
    "This notebook analyzes model evaluation results to understand:\n",
    "- Where models are making mistakes\n",
    "- Answer distribution patterns\n",
    "- Most common confusions\n",
    "- Per-class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Path to your results file\nRESULTS_FILE = \"./output/results_paligemma_Controlled_Images_B_None_fouroption_False.json\"\n\n# Dataset configuration - UPDATE THESE to match your results file\nDATASET_NAME = \"Controlled_Images_B\"  # Change to match your dataset\nDATA_DIR = \"../data\"  # Path to data directory\n\n# You can also compare multiple models\n# RESULTS_FILES = {\n#     \"PaliGemma\": \"./output/results_paligemma_Controlled_Images_B_None_fouroption_False.json\",\n#     \"Qwen2.5-VL\": \"./output/results_qwen_vllm_Controlled_Images_B_None_fouroption_False.json\",\n# }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spatial_answer(generation):\n",
    "    \"\"\"\n",
    "    Extract spatial relation from model generation.\n",
    "    \n",
    "    Args:\n",
    "        generation: Raw model output string\n",
    "    \n",
    "    Returns:\n",
    "        Extracted spatial relation (lowercase) or 'unknown'\n",
    "    \"\"\"\n",
    "    gen_lower = generation.lower().strip()\n",
    "    \n",
    "    # List of possible spatial relations\n",
    "    spatial_relations = [\n",
    "        'left', 'right', 'above', 'below', 'top', 'bottom',\n",
    "        'on', 'under', 'front', 'behind', 'in-front'\n",
    "    ]\n",
    "    \n",
    "    # Try to find exact matches first\n",
    "    for relation in spatial_relations:\n",
    "        if relation == gen_lower or f' {relation} ' in f' {gen_lower} ':\n",
    "            return relation\n",
    "    \n",
    "    # Try to find relations at the beginning or end\n",
    "    for relation in spatial_relations:\n",
    "        if gen_lower.startswith(relation + ' ') or gen_lower.endswith(' ' + relation):\n",
    "            return relation\n",
    "    \n",
    "    # If no exact match, return the generation (truncated)\n",
    "    return gen_lower[:20] if gen_lower else 'unknown'\n",
    "\n",
    "\n",
    "# Load results\n",
    "with open(RESULTS_FILE, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(results)} results\")\n",
    "print(f\"\\nFirst result example:\")\n",
    "print(json.dumps(results[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Clean Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and ground truth\n",
    "predicted_answers = []\n",
    "golden_answers = []\n",
    "correct_predictions = []\n",
    "raw_generations = []\n",
    "\n",
    "for result in results:\n",
    "    raw_gen = result['Generation']\n",
    "    pred = extract_spatial_answer(raw_gen)\n",
    "    gold = result['Golden'].lower() if isinstance(result['Golden'], str) else result['Golden'][0].lower()\n",
    "    \n",
    "    raw_generations.append(raw_gen)\n",
    "    predicted_answers.append(pred)\n",
    "    golden_answers.append(gold)\n",
    "    correct_predictions.append(pred == gold)\n",
    "\n",
    "# Create DataFrame for easier analysis\n",
    "df = pd.DataFrame({\n",
    "    'prompt': [r['Prompt'] for r in results],\n",
    "    'raw_generation': raw_generations,\n",
    "    'predicted': predicted_answers,\n",
    "    'golden': golden_answers,\n",
    "    'correct': correct_predictions\n",
    "})\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(df)\n",
    "correct = df['correct'].sum()\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {total}\")\n",
    "print(f\"Correct predictions: {correct}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class Accuracy\n",
    "\n",
    "Which spatial relations is the model struggling with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class accuracy\n",
    "per_class = df.groupby('golden').agg({\n",
    "    'correct': ['sum', 'count', 'mean']\n",
    "}).round(4)\n",
    "per_class.columns = ['correct', 'total', 'accuracy']\n",
    "per_class['accuracy'] = per_class['accuracy'] * 100\n",
    "per_class = per_class.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(per_class)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(per_class.index, per_class['accuracy'], color='steelblue', alpha=0.8)\n",
    "ax.set_xlabel('Spatial Relation (Golden Answer)', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title(f'Per-Class Accuracy\\nOverall Accuracy: {accuracy:.2f}%', fontsize=14)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.axhline(y=accuracy, color='r', linestyle='--', alpha=0.5, label=f'Overall: {accuracy:.1f}%')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, (idx, row) in zip(bars, per_class.iterrows()):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "            f'n={int(row[\"total\"])}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Distribution\n",
    "\n",
    "Compare golden vs predicted answer distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count distributions\n",
    "golden_dist = Counter(golden_answers)\n",
    "predicted_dist = Counter(predicted_answers)\n",
    "\n",
    "print(\"\\nGolden Answer Distribution:\")\n",
    "for answer, count in golden_dist.most_common():\n",
    "    pct = 100 * count / len(golden_answers)\n",
    "    print(f\"  {answer:15s}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\nPredicted Answer Distribution:\")\n",
    "for answer, count in predicted_dist.most_common():\n",
    "    pct = 100 * count / len(predicted_answers)\n",
    "    print(f\"  {answer:15s}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Visualize side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Golden distribution\n",
    "ax1.bar(golden_dist.keys(), golden_dist.values(), color='green', alpha=0.6)\n",
    "ax1.set_xlabel('Spatial Relation', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_title('Golden Answer Distribution', fontsize=14)\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Predicted distribution\n",
    "ax2.bar(predicted_dist.keys(), predicted_dist.values(), color='orange', alpha=0.6)\n",
    "ax2.set_xlabel('Spatial Relation', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Predicted Answer Distribution', fontsize=14)\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "What does the model predict when it's wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix\n",
    "all_labels = sorted(set(golden_answers + predicted_answers))\n",
    "confusion = np.zeros((len(all_labels), len(all_labels)))\n",
    "label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "\n",
    "for gold, pred in zip(golden_answers, predicted_answers):\n",
    "    confusion[label_to_idx[gold], label_to_idx[pred]] += 1\n",
    "\n",
    "# Normalize by row (golden answer) to get percentages\n",
    "confusion_norm = confusion / (confusion.sum(axis=1, keepdims=True) + 1e-10) * 100\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.heatmap(confusion_norm, annot=True, fmt='.1f', cmap='YlOrRd',\n",
    "            xticklabels=all_labels, yticklabels=all_labels,\n",
    "            cbar_kws={'label': 'Percentage (%)'}, ax=ax)\n",
    "ax.set_xlabel('Predicted Answer', fontsize=12)\n",
    "ax.set_ylabel('Golden Answer', fontsize=12)\n",
    "ax.set_title('Confusion Matrix (% of each golden answer)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also show absolute counts\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.heatmap(confusion, annot=True, fmt='.0f', cmap='Blues',\n",
    "            xticklabels=all_labels, yticklabels=all_labels,\n",
    "            cbar_kws={'label': 'Count'}, ax=ax)\n",
    "ax.set_xlabel('Predicted Answer', fontsize=12)\n",
    "ax.set_ylabel('Golden Answer', fontsize=12)\n",
    "ax.set_title('Confusion Matrix (Absolute Counts)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Confusion Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common confusions for each class\n",
    "print(\"\\nMost Common Confusion Patterns:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for gold in sorted(set(golden_answers)):\n",
    "    gold_mask = df['golden'] == gold\n",
    "    predictions = df[gold_mask]['predicted'].values\n",
    "    pred_counts = Counter(predictions)\n",
    "    \n",
    "    print(f\"\\nWhen golden answer is '{gold}' (n={sum(gold_mask)}):\")\n",
    "    for pred, count in pred_counts.most_common(5):\n",
    "        pct = 100 * count / sum(gold_mask)\n",
    "        is_correct = \"✓\" if pred == gold else \"✗\"\n",
    "        print(f\"  {is_correct} {pred:15s}: {count:4d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Load dataset to get original caption options  \nimport sys\nsys.path.append('..')\nfrom dataset_zoo.aro_datasets import (\n    get_controlled_images_a, get_controlled_images_b,\n    get_coco_qa_one_obj, get_coco_qa_two_obj,\n    get_vg_qa_one_obj, get_vg_qa_two_obj\n)\n\n# Dataset name mapping\ndataset_loaders = {\n    \"Controlled_Images_A\": get_controlled_images_a,\n    \"Controlled_Images_B\": get_controlled_images_b,\n    \"COCO_QA_one_obj\": get_coco_qa_one_obj,\n    \"COCO_QA_two_obj\": get_coco_qa_two_obj,\n    \"VG_QA_one_obj\": get_vg_qa_one_obj,\n    \"VG_QA_two_obj\": get_vg_qa_two_obj,\n}\n\n# Load the dataset\nprint(f\"Loading dataset: {DATASET_NAME}\")\nif DATASET_NAME not in dataset_loaders:\n    print(f\"Error: Unknown dataset {DATASET_NAME}\")\n    print(f\"Available datasets: {list(dataset_loaders.keys())}\")\nelse:\n    dataset = dataset_loaders[DATASET_NAME](image_preprocess=None, download=False, root_dir=DATA_DIR)\n    print(f\"Dataset loaded: {len(dataset)} samples\")\n    \n    # Show sample\n    print(\"\\nSample from dataset:\")\n    sample = dataset[0]\n    print(f\"Caption options: {sample['caption_options']}\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def extract_objects_from_caption(caption, dataset_type):\n    \"\"\"\n    Extract objects from caption based on dataset format.\n    \n    Args:\n        caption: Caption string from dataset\n        dataset_type: Type of dataset (controlled, coco, vg)\n    \n    Returns:\n        dict with 'object1' and 'object2' (if exists)\n    \"\"\"\n    caption_lower = caption.lower()\n    \n    if dataset_type in ['Controlled_Images_A', 'Controlled_Images_B']:\n        # Format: \"A {obj1} {relation} a {obj2}\"\n        # e.g., \"A beer bottle on a armchair\"\n        words = caption_lower.split()\n        if len(words) >= 4:\n            obj1 = words[1]  # Position [1]\n            obj2 = words[-1]  # Position [-1]\n            return {'object1': obj1, 'object2': obj2}\n    \n    elif dataset_type in ['COCO_QA_one_obj', 'VG_QA_one_obj']:\n        # Format: \"A photo of a {object} on the {position}\"\n        # e.g., \"A photo of a laptop on the left\"\n        if caption_lower.startswith('a photo of a'):\n            text = caption_lower.replace('a photo of a ', '')\n            # Remove position part\n            for pos in [' on the left', ' on the right']:\n                if text.endswith(pos):\n                    obj = text.replace(pos, '').strip()\n                    return {'object1': obj, 'object2': None}\n    \n    elif dataset_type in ['COCO_QA_two_obj', 'VG_QA_two_obj']:\n        # Format: \"A photo of a {obj1} to the {position} of a {obj2}\"\n        # e.g., \"A photo of a metal lamp to the right of a laptop\"\n        if caption_lower.startswith('a photo of a'):\n            text = caption_lower.replace('a photo of a ', '')\n            if ' to the right of a ' in text:\n                parts = text.split(' to the right of a ')\n                return {'object1': parts[0].strip(), 'object2': parts[1].strip()}\n            elif ' to the left of a ' in text:\n                parts = text.split(' to the left of a ')\n                return {'object1': parts[0].strip(), 'object2': parts[1].strip()}\n    \n    return {'object1': None, 'object2': None}\n\n\n# Extract objects for all samples\nprint(\"Extracting objects from dataset...\")\ndf['object1'] = None\ndf['object2'] = None\n\nfor idx in range(len(df)):\n    if idx < len(dataset):\n        sample = dataset[idx]\n        # Get the correct caption (first one in caption_options)\n        correct_caption = sample['caption_options'][0]\n        extracted = extract_objects_from_caption(correct_caption, DATASET_NAME)\n        df.at[idx, 'object1'] = extracted['object1']\n        df.at[idx, 'object2'] = extracted['object2']\n\nprint(f\"Extracted objects for {len(df)} samples\")\nprint(\"\\nSample extractions:\")\nprint(df[['prompt', 'object1', 'object2', 'golden', 'correct']].head(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Per-Object1 Accuracy (the primary object being described)\nprint(\"=\"*60)\nprint(\"PER-OBJECT ACCURACY (Object 1)\")\nprint(\"=\"*60)\n\nobj1_stats = df.groupby('object1').agg({\n    'correct': ['sum', 'count', 'mean']\n}).round(4)\nobj1_stats.columns = ['correct', 'total', 'accuracy']\nobj1_stats['accuracy'] = obj1_stats['accuracy'] * 100\nobj1_stats = obj1_stats.sort_values('accuracy', ascending=False)\n\nprint(\"\\nObject 1 Accuracy:\")\nprint(obj1_stats)\n\n# Visualize - show top 20 objects by frequency\ntop_objects = obj1_stats.nlargest(20, 'total')\n\nfig, ax = plt.subplots(figsize=(14, 8))\nbars = ax.barh(range(len(top_objects)), top_objects['accuracy'], color='steelblue', alpha=0.8)\nax.set_yticks(range(len(top_objects)))\nax.set_yticklabels(top_objects.index)\nax.set_xlabel('Accuracy (%)', fontsize=12)\nax.set_ylabel('Object', fontsize=12)\nax.set_title(f'Top 20 Objects by Frequency - Accuracy\\n(Primary Object in Prompt)', fontsize=14, pad=20)\nax.set_xlim(0, 100)\nax.axvline(x=accuracy, color='r', linestyle='--', alpha=0.5, label=f'Overall: {accuracy:.1f}%')\n\n# Add count labels\nfor i, (idx, row) in enumerate(top_objects.iterrows()):\n    ax.text(row['accuracy'] + 2, i, f\"n={int(row['total'])}\", \n            va='center', fontsize=9)\n\nax.legend()\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Per-Object2 Accuracy (the reference object)\nprint(\"=\"*60)\nprint(\"PER-OBJECT ACCURACY (Object 2 - Reference Object)\")\nprint(\"=\"*60)\n\n# Filter out rows where object2 is None\ndf_with_obj2 = df[df['object2'].notna()].copy()\n\nif len(df_with_obj2) > 0:\n    obj2_stats = df_with_obj2.groupby('object2').agg({\n        'correct': ['sum', 'count', 'mean']\n    }).round(4)\n    obj2_stats.columns = ['correct', 'total', 'accuracy']\n    obj2_stats['accuracy'] = obj2_stats['accuracy'] * 100\n    obj2_stats = obj2_stats.sort_values('accuracy', ascending=False)\n    \n    print(\"\\nObject 2 Accuracy:\")\n    print(obj2_stats)\n    \n    # Visualize - show top 20 objects by frequency\n    top_objects2 = obj2_stats.nlargest(20, 'total')\n    \n    fig, ax = plt.subplots(figsize=(14, 8))\n    bars = ax.barh(range(len(top_objects2)), top_objects2['accuracy'], color='coral', alpha=0.8)\n    ax.set_yticks(range(len(top_objects2)))\n    ax.set_yticklabels(top_objects2.index)\n    ax.set_xlabel('Accuracy (%)', fontsize=12)\n    ax.set_ylabel('Object', fontsize=12)\n    ax.set_title(f'Top 20 Objects by Frequency - Accuracy\\n(Reference Object in Prompt)', fontsize=14, pad=20)\n    ax.set_xlim(0, 100)\n    ax.axvline(x=accuracy, color='r', linestyle='--', alpha=0.5, label=f'Overall: {accuracy:.1f}%')\n    \n    # Add count labels\n    for i, (idx, row) in enumerate(top_objects2.iterrows()):\n        ax.text(row['accuracy'] + 2, i, f\"n={int(row['total'])}\", \n                va='center', fontsize=9)\n    \n    ax.legend()\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No samples with object2 found.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Object Pair Analysis (for two-object scenarios)\nprint(\"=\"*60)\nprint(\"OBJECT PAIR ANALYSIS\")\nprint(\"=\"*60)\n\nif len(df_with_obj2) > 0:\n    # Create object pair identifier\n    df_with_obj2['object_pair'] = df_with_obj2['object1'] + ' + ' + df_with_obj2['object2']\n    \n    pair_stats = df_with_obj2.groupby('object_pair').agg({\n        'correct': ['sum', 'count', 'mean']\n    }).round(4)\n    pair_stats.columns = ['correct', 'total', 'accuracy']\n    pair_stats['accuracy'] = pair_stats['accuracy'] * 100\n    pair_stats = pair_stats.sort_values('total', ascending=False)\n    \n    print(\"\\nTop 20 Most Frequent Object Pairs:\")\n    print(pair_stats.head(20))\n    \n    # Find worst performing pairs (with at least 3 samples)\n    worst_pairs = pair_stats[pair_stats['total'] >= 3].sort_values('accuracy').head(15)\n    \n    print(\"\\nWorst Performing Object Pairs (n >= 3):\")\n    print(worst_pairs)\n    \n    # Visualize worst performing pairs\n    if len(worst_pairs) > 0:\n        fig, ax = plt.subplots(figsize=(12, 8))\n        bars = ax.barh(range(len(worst_pairs)), worst_pairs['accuracy'], color='crimson', alpha=0.7)\n        ax.set_yticks(range(len(worst_pairs)))\n        ax.set_yticklabels(worst_pairs.index, fontsize=10)\n        ax.set_xlabel('Accuracy (%)', fontsize=12)\n        ax.set_ylabel('Object Pair', fontsize=12)\n        ax.set_title('Worst Performing Object Pairs\\n(Minimum 3 samples)', fontsize=14, pad=20)\n        ax.set_xlim(0, 100)\n        ax.axvline(x=accuracy, color='r', linestyle='--', alpha=0.5, label=f'Overall: {accuracy:.1f}%')\n        \n        # Add count labels\n        for i, (idx, row) in enumerate(worst_pairs.iterrows()):\n            ax.text(row['accuracy'] + 2, i, f\"n={int(row['total'])}\", \n                    va='center', fontsize=9)\n        \n        ax.legend()\n        plt.tight_layout()\n        plt.show()\nelse:\n    print(\"No two-object samples found.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Specific Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View incorrect predictions\n",
    "incorrect_df = df[~df['correct']].copy()\n",
    "print(f\"\\nTotal incorrect predictions: {len(incorrect_df)}\")\n",
    "\n",
    "# Show first few errors\n",
    "print(\"\\nFirst 10 errors:\")\n",
    "incorrect_df[['prompt', 'raw_generation', 'predicted', 'golden']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Specific Confusion Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a specific confusion to investigate\n",
    "GOLD_ANSWER = 'left'  # Change this to investigate different confusions\n",
    "PRED_ANSWER = 'right'  # Change this to investigate different confusions\n",
    "\n",
    "confusion_mask = (df['golden'] == GOLD_ANSWER) & (df['predicted'] == PRED_ANSWER)\n",
    "confusion_cases = df[confusion_mask]\n",
    "\n",
    "print(f\"\\nCases where golden='{GOLD_ANSWER}' but predicted='{PRED_ANSWER}': {len(confusion_cases)}\")\n",
    "print(\"\\nExamples:\")\n",
    "for idx, row in confusion_cases.head(5).iterrows():\n",
    "    print(f\"\\nPrompt: {row['prompt']}\")\n",
    "    print(f\"Generated: {row['raw_generation']}\")\n",
    "    print(f\"Predicted: {row['predicted']} | Golden: {row['golden']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Generation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the raw generations to understand model behavior\n",
    "print(\"\\nSample raw generations (correct predictions):\")\n",
    "for idx, row in df[df['correct']].sample(min(5, len(df[df['correct']]))).iterrows():\n",
    "    print(f\"\\nGolden: {row['golden']}\")\n",
    "    print(f\"Raw: {row['raw_generation']}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n\\nSample raw generations (incorrect predictions):\")\n",
    "for idx, row in df[~df['correct']].sample(min(5, len(df[~df['correct']]))).iterrows():\n",
    "    print(f\"\\nGolden: {row['golden']} | Predicted: {row['predicted']}\")\n",
    "    print(f\"Raw: {row['raw_generation']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to JSON\n",
    "summary = {\n",
    "    'overall': {\n",
    "        'total': total,\n",
    "        'correct': int(correct),\n",
    "        'accuracy': float(accuracy)\n",
    "    },\n",
    "    'per_class_accuracy': per_class.to_dict('index'),\n",
    "    'golden_distribution': dict(golden_dist),\n",
    "    'predicted_distribution': dict(predicted_dist)\n",
    "}\n",
    "\n",
    "output_file = RESULTS_FILE.replace('.json', '_analysis.json')\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nAnalysis saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}